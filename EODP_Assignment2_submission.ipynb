{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1a.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import textdistance\n",
    "\n",
    "# read corresponding files\n",
    "amazon_small = pd.read_csv('amazon_small.csv')\n",
    "google_small = pd.read_csv('google_small.csv')\n",
    "amazon_google_truth_small = pd.read_csv('amazon_google_truth_small.csv')\n",
    "\n",
    "# create a list to install all the product pairs\n",
    "total_pair = []\n",
    "\n",
    "# select columns separately from two datasets\n",
    "for i in range(amazon_small.shape[0]):\n",
    "    max_num = 0\n",
    "    \n",
    "    amazon_title = amazon_small.loc[i][\"title\"]\n",
    "    amazon_des = amazon_small[\"description\"]\n",
    "    amazon_manu = amazon_small[\"manufacturer\"]\n",
    "    amazon_price = amazon_small[\"price\"]\n",
    "    \n",
    "    for j in range(google_small.shape[0]):\n",
    "        google_title = google_small.loc[j][\"name\"]\n",
    "        google_des = google_small[\"description\"]\n",
    "        google_manu = google_small[\"manufacturer\"]\n",
    "        google_price = google_small[\"price\"]\n",
    "        \n",
    "        t1 = amazon_title.split()\n",
    "        t2 = google_title.split()\n",
    "        \n",
    "        # calculate the difference      \n",
    "        sim1 = textdistance.jaccard(t1, t2)\n",
    "        sim2 = textdistance.jaccard(str(amazon_des[i]), str(google_price[j]))\n",
    "        sim3 = textdistance.jaccard(str(amazon_manu[i]), str(google_manu[j]))\n",
    "        sim4 = textdistance.jaccard(str(amazon_price[i]), str(google_price[j]))\n",
    "        \n",
    "        # weight the difference    \n",
    "        total_sim = sim1*0.3 + sim2*0.2 + sim3*0.1 + sim4*0.4\n",
    "    \n",
    "        # find the biggest similarity under each amazon id\n",
    "        if total_sim > max_num:\n",
    "            max_num = total_sim\n",
    "            amazonid = amazon_small.loc[i][\"idAmazon\"]\n",
    "            googleid = google_small.loc[i][\"idGoogleBase\"]\n",
    "\n",
    "    # create a threshold \n",
    "    if max_num > 0.40:\n",
    "        total_pair.append([amazonid, googleid])\n",
    "\n",
    "# create a csv file\n",
    "with open (\"task1a.csv\", 'a', newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['idAmazon', 'idGoogleBase'])\n",
    "    for i in range(len(total_pair)):\n",
    "        writer.writerow(total_pair[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1b.py\n",
    "# read all corresponding csv files\n",
    "amazon = pd.read_csv('amazon.csv')\n",
    "google = pd.read_csv('google.csv')\n",
    "amazon_google_truth = pd.read_csv('amazon_google_truth.csv')\n",
    "# create a function to calculate gdp in aud\n",
    "def gdp_to_aud(price):\n",
    "    price = float(price.split()[0])*1.88\n",
    "    return price\n",
    "\n",
    "# convert gdp to aud\n",
    "for i in range(google.shape[0]):\n",
    "    try:\n",
    "        float(google.loc[i][\"price\"])\n",
    "    except:\n",
    "        google.loc[i][\"price\"] = gdp_to_aud(google.loc[i][\"price\"])\n",
    "    try:\n",
    "        google[\"price\"][i] = float(google[\"price\"][i])\n",
    "    except:\n",
    "        google[\"price\"][i] = np.nan\n",
    "\n",
    "# create bin size and range\n",
    "numlist = [num for num in range(0, 500, 20)]\n",
    "for num in range(500, 1000, 100):\n",
    "    numlist.append(num)\n",
    "for num in range(1000, 10000, 1000):\n",
    "    numlist.append(num)\n",
    "for num in range(10000, 60000, 10000):\n",
    "    numlist.append(num)\n",
    "label = [num for num in range(len(numlist)-1)]\n",
    "\n",
    "google[\"price\"] = pd.cut(google[\"price\"], bins = numlist, labels = label)\n",
    "amazon[\"price\"] = pd.cut(amazon[\"price\"], bins = numlist, labels = label)\n",
    "\n",
    "block_key = set()\n",
    "for i in amazon[\"price\"]:\n",
    "    block_key.add(str(i))\n",
    "for i in google[\"price\"]:\n",
    "    block_key.add(str(i))  \n",
    "\n",
    "# put everything into the blocks\n",
    "block_d = {}\n",
    "for keys in block_key:\n",
    "    d = {}\n",
    "    d[\"Amazon\"] = []\n",
    "    d[\"Google\"] = []\n",
    "    block_d[keys] = d\n",
    "block_d[np.nan] = d\n",
    "\n",
    "for i in range(amazon.shape[0]):\n",
    "    try:\n",
    "        block_d[amazon.loc[i][\"price\"]][\"Amazon\"].append(amazon.loc[i][\"idAmazon\"])\n",
    "    except:\n",
    "        block_d[np.nan][\"Amazon\"].append(amazon.loc[i][\"idAmazon\"])\n",
    "for i in range(google.shape[0]):\n",
    "    try:\n",
    "        block_d[google.loc[i][\"price\"]][\"Google\"].append(google.loc[i][\"id\"])\n",
    "    except:\n",
    "        block_d[np.nan][\"Google\"].append(google.loc[i][\"id\"])   \n",
    "\n",
    "# save the dataframe to csv file\n",
    "amazon_df = pd.DataFrame({\"blocking_keys\": amazon[\"price\"], \"product_id\": amazon[\"idAmazon\"]})\n",
    "google_df = pd.DataFrame({\"blocking_keys\": google[\"price\"], \"product_id\": google[\"id\"]})\n",
    "amazon_df.to_csv(\"amazon_blocks.csv\")\n",
    "google_df.to_csv(\"google_blocks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of decision tree: 72.131%\n",
      "Accuracy of k-nn (k=5):81.967%\n",
      "Accuracy of k-nn (k=10):83.607%\n"
     ]
    }
   ],
   "source": [
    "# task2a.py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "world = pd.read_csv(\"world.csv\")\n",
    "life = pd.read_csv(\"life.csv\")\n",
    "life_world = life.merge(world.iloc[:,2:], how = 'left')\n",
    "life_world = life_world.replace(\"..\", np.nan)\n",
    "mean_list = []\n",
    "std_list = []\n",
    "\n",
    "data = life_world.iloc[:, 4:]\n",
    "y = life_world[\"Life expectancy at birth (years)\"]\n",
    "\n",
    "median_list = [i for i in data.median(skipna=True)]\n",
    "\n",
    "for column in data.columns:\n",
    "    median = data[column].median()\n",
    "    data[column].fillna(median, inplace = True)\n",
    "#     X_test[column].fillna(median, inplace = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, train_size = 2/3, test_size=1/3, random_state = 100)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardised = data.copy()\n",
    "standardised.iloc[:, :] = scaler.fit_transform(standardised.iloc[:, :])\n",
    "\n",
    "mean = pd.DataFrame(data = standardised.mean())\n",
    "\n",
    "for i in range(len(standardised.mean())):\n",
    "    row = mean.iloc[i].values\n",
    "    mean_list.append(float(row))\n",
    "    \n",
    "std = pd.DataFrame(data = standardised.std())\n",
    "\n",
    "for i in range(len(std)):\n",
    "    row = std.iloc[i].values\n",
    "    std_list.append(float(row))\n",
    "    \n",
    "X_train = StandardScaler(with_mean = True, with_std = True).fit_transform(X_train)\n",
    "X_test = StandardScaler(with_mean = True, with_std = True).fit_transform(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "clf.fit(X_train, y_train)\n",
    "val1 = round(clf.score(X_test, y_test)*100,3)\n",
    "print(f'Accuracy of decision tree: {val1}%')\n",
    "\n",
    "# knn = 5\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train, y_train)\n",
    "val2 = round(knn5.score(X_test, y_test)*100,3)\n",
    "print(f'Accuracy of k-nn (k=5):{val2}%')\n",
    "\n",
    "# knn = 10\n",
    "knn10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn10.fit(X_train, y_train)\n",
    "val3 = round(knn10.score(X_test, y_test)*100, 3)\n",
    "print(f'Accuracy of k-nn (k=10):{val3}%')\n",
    "\n",
    "# save the dataframe to csv file\n",
    "task2a = pd.DataFrame({\"feature\": data.columns, \"median\": median_list, \"mean\": mean_list, \"variance\": std_list})\n",
    "task2a.to_csv(\"task2a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy(MI): 85.246%\n",
      "Test Accuracy(MI): 78.689%\n",
      "Accuracy of k-nn (k=5) by using MI:78.689%\n",
      "===========================================\n",
      "Train Accuracy: 73.77%\n",
      "Test Accuracy: 72.131%\n",
      "Accuracy of k-nn (k=5) by using AMI:72.131%\n",
      "===========================================\n",
      "Train Accuracy: 85.246%\n",
      "Test Accuracy: 78.689%\n",
      "Accuracy of k-nn (k=5) by using NMI:78.689%\n",
      "===========================================\n",
      "Train Accuracy: 82.787%\n",
      "Test Accuracy: 80.328%\n",
      "Accuracy of k-nn (k=5) by using pca:80.328%\n",
      "===========================================\n",
      "Train Accuracy: 81.967%\n",
      "Test Accuracy: 75.41%\n",
      "Accuracy of k-nn (k=5) by selecting the first four columns:75.41%\n"
     ]
    }
   ],
   "source": [
    "# task2b.py\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# find feature * feature\n",
    "poly = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\n",
    "new_features = poly.fit_transform(data)\n",
    "\n",
    "new_features = pd.DataFrame(new_features)\n",
    "\n",
    "# the name of the columns\n",
    "col_name = []\n",
    "for i in range(data.shape[1]):\n",
    "    for j in range(i, data.shape[1]):\n",
    "        if i != j:\n",
    "            col_name.append(f\"f{i}*f{j}\")\n",
    "            \n",
    "full_col_name = []\n",
    "for i in data.columns:\n",
    "    full_col_name.append(i)\n",
    "for i in col_name:\n",
    "    full_col_name.append(i)\n",
    "    \n",
    "new_features.columns = full_col_name\n",
    "\n",
    "# add one column to the dataframe\n",
    "kmeans = KMeans(n_clusters=3).fit(new_features)\n",
    "new_features[\"f_clusterabel\"] = kmeans.labels_\n",
    "\n",
    "shape = data.shape[1]\n",
    "# find the accuracy by MI\n",
    "\n",
    "# calculate MI for each feature\n",
    "MI_dict = {}\n",
    "MI_name = []\n",
    "feature_list = []\n",
    "for i in range(shape):\n",
    "    for j in range(i, shape):\n",
    "        if i != j:\n",
    "            MI_dict[f\"f{i}*f{j}\"] = mutual_info_score(data.iloc[:,i].astype('float'), data.iloc[:,j].astype('float'))\n",
    "\n",
    "MI_dict = sorted(MI_dict.items(),key = lambda x:x[1],reverse = True)\n",
    "MI_dict_most = MI_dict[:4]\n",
    "\n",
    "for i in MI_dict_most:\n",
    "    MI_name.append(i[0])\n",
    "for i in MI_name:\n",
    "    feature_list.append(new_features[i])\n",
    "feature_list = pd.DataFrame(feature_list).transpose()\n",
    "\n",
    "# split training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_list, y, train_size = 2/3, test_size=1/3, random_state = 100)\n",
    "\n",
    "# train knn\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train, y_train)\n",
    "\n",
    "# test accuracy\n",
    "y_test_pred = knn5.predict(X_test)\n",
    "test = round(accuracy_score(y_test, y_test_pred)*100, 3)\n",
    "\n",
    "# train accuracy\n",
    "y_train_pred = knn5.predict(X_train)\n",
    "train = round(accuracy_score(y_train, y_train_pred)*100, 3)\n",
    "\n",
    "val1 = round(knn5.score(X_test, y_test)*100,3)\n",
    "\n",
    "print(f'Train Accuracy(MI): {train}%')\n",
    "print(f'Test Accuracy(MI): {test}%')\n",
    "print(f'Accuracy of k-nn (k=5) by using MI:{val1}%')\n",
    "print(\"===========================================\")\n",
    "\n",
    "# find accruacy by AMI\n",
    "AMI_dict = {}\n",
    "AMI_name = []\n",
    "feature_list = []\n",
    "for i in range(shape):\n",
    "    for j in range(i, shape):\n",
    "        if i != j:\n",
    "            AMI_dict[f\"f{i}*f{j}\"] = adjusted_mutual_info_score(data.iloc[:,i].astype('float'), data.iloc[:,j].astype('float'))\n",
    "\n",
    "AMI_dict = sorted(AMI_dict.items(),key = lambda x:x[1],reverse = True)\n",
    "AMI_dict_most = AMI_dict[:4]\n",
    "\n",
    "for i in AMI_dict_most:\n",
    "    AMI_name.append(i[0])\n",
    "for i in AMI_name:\n",
    "    feature_list.append(new_features[i])\n",
    "feature_list = pd.DataFrame(feature_list).transpose()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_list, y, train_size = 2/3, test_size=1/3, random_state = 100)\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train, y_train)\n",
    "y_test_pred = knn5.predict(X_test)\n",
    "test = round(accuracy_score(y_test, y_test_pred)*100, 3)\n",
    "y_train_pred = knn5.predict(X_train)\n",
    "train = round(accuracy_score(y_train, y_train_pred)*100, 3)\n",
    "\n",
    "val1 = round(knn5.score(X_test, y_test)*100,3)\n",
    "\n",
    "print(f'Train Accuracy: {train}%')\n",
    "print(f'Test Accuracy: {test}%')\n",
    "print(f'Accuracy of k-nn (k=5) by using AMI:{val1}%')\n",
    "print(\"===========================================\")\n",
    "\n",
    "# find accuracy by NMI\n",
    "NMI_dict = {}\n",
    "NMI_name = []\n",
    "feature_list = []\n",
    "for i in range(shape):\n",
    "    for j in range(i, shape):\n",
    "        if i != j:\n",
    "            NMI_dict[f\"f{i}*f{j}\"] = normalized_mutual_info_score(data.iloc[:,i].astype('float'), data.iloc[:,j].astype('float'))\n",
    "\n",
    "NMI_dict = sorted(NMI_dict.items(),key = lambda x:x[1],reverse = True)\n",
    "NMI_dict_most = NMI_dict[:4]\n",
    "\n",
    "for i in NMI_dict_most:\n",
    "    NMI_name.append(i[0])\n",
    "for i in NMI_name:\n",
    "    feature_list.append(new_features[i])\n",
    "feature_list = pd.DataFrame(feature_list).transpose()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_list, y, train_size = 2/3, test_size=1/3, random_state = 100)\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train, y_train)\n",
    "y_test_pred = knn5.predict(X_test)\n",
    "test = round(accuracy_score(y_test, y_test_pred)*100, 3)\n",
    "y_train_pred = knn5.predict(X_train)\n",
    "train = round(accuracy_score(y_train, y_train_pred)*100, 3)\n",
    "\n",
    "val1 = round(knn5.score(X_test, y_test)*100,3)\n",
    "\n",
    "print(f'Train Accuracy: {train}%')\n",
    "print(f'Test Accuracy: {test}%')\n",
    "print(f'Accuracy of k-nn (k=5) by using NMI:{val1}%')\n",
    "print(\"===========================================\")\n",
    "\n",
    "# find accruacy by pca\n",
    "pca = PCA(n_components=4)\n",
    "X_reduced = pd.DataFrame(pca.fit_transform(new_features))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, train_size = 2/3, test_size=1/3, random_state = 100)\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train, y_train)\n",
    "y_test_pred = knn5.predict(X_test)\n",
    "test = round(accuracy_score(y_test, y_test_pred)*100, 3)\n",
    "y_train_pred = knn5.predict(X_train)\n",
    "train = round(accuracy_score(y_train, y_train_pred)*100, 3)\n",
    "\n",
    "val2 = round(knn5.score(X_test, y_test)*100,3)\n",
    "\n",
    "print(f'Train Accuracy: {train}%')\n",
    "print(f'Test Accuracy: {test}%')\n",
    "print(f'Accuracy of k-nn (k=5) by using pca:{val2}%')\n",
    "print(\"===========================================\")\n",
    "\n",
    "# find accuracy by the first four columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 0:4], y, train_size = 2/3, test_size=1/3, random_state = 100)\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train, y_train)\n",
    "y_test_pred = knn5.predict(X_test)\n",
    "test = round(accuracy_score(y_test, y_test_pred)*100, 3)\n",
    "y_train_pred = knn5.predict(X_train)\n",
    "train = round(accuracy_score(y_train, y_train_pred)*100, 3)\n",
    "\n",
    "val3 = round(knn5.score(X_test, y_test)*100, 3)\n",
    "\n",
    "print(f'Train Accuracy: {train}%')\n",
    "print(f'Test Accuracy: {test}%')\n",
    "print(f'Accuracy of k-nn (k=5) by selecting the first four columns:{val3}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
